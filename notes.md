* Expertise in a number of the following technologies R, Python, SQL, Spark, Hadoop, Tensorflow, AWS, GitLab, GitHub, Shiny, Qlik, Tableau, Java
* Understanding and having being involved in projects that implied the use of classical statistical and machine learning models such as predictive analytics, multivariate regression (including Mix Models), Time Series, Clustering methods, Optimization algorithms, RF, Boosting, SVM, Neural Networks, NLP, Causal Models, Bayesian Networks, etc.

* portable, reproducible analysis

* cloud computing

* stratification/segmentation methods [here](https://www.qualtrics.com/uk/experience-management/brand/market-segmentation/)

*  Proficiency with making data easily accessible to collaborators (e.g. Shiny, Dash) 

* workflow management systems (e.g. Snakemake, Nextflow, Cromwell/WDL, Knime) [see here](http://blog.booleanbiotech.com/nextflow-snakemake-reflow.html) [and here](http://www.opiniomics.org/the-three-technologies-bioinformaticians-need-to-be-using-right-now/)

* hadoop (mature "ecosystem", two components: 1) MapReduce - an indexing was to do work on cluster (perhaps increasingly old), 2) HDFS - how to store the data as a file system) and spark (new) are the competing frameworks for dealing with big data (spark can process HDFS). They compete as a processing engine. Spark better for streaming. Hadoop-MapReduce was not built for realtime processing. https://www.youtube.com/watch?v=xDpvyu0w0C8

* The dplyr methods convert your R code into SQL code before passing it to Spark
